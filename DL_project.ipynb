{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/!\\ FIRST THING TO DO :\n",
    "\n",
    "EXECUTION --> MODIFIER LE TYPE D'EXECUTION --> GPU (Accélérateur matériel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow 2.0 on Google Collab if needed\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify GPU usage. You should see \n",
    "\n",
    "```\n",
    "Number of GPUs available : 1\n",
    "Only GPU number 0 used\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_GPU = 0\n",
    "\n",
    "import tensorflow as tf\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "print('Number of GPUs available :', len(gpus))\n",
    "\n",
    "tf.config.experimental.set_visible_devices(gpus[num_GPU], 'GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[num_GPU], True)\n",
    "print('Only GPU number', num_GPU, 'used')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "\n",
    "In this project, the goal is to create a set of NN-based transmitter and receiver that beats a conventional QAM system. \n",
    "\n",
    "The performance metric is the rate of the transmission. The rate of a transmission is the maximum number of bits that could be sent through the transmitter-receiver if perfect channel coding was used (the bits are coded/decoded before/after the transmissions). In this project, we won't use channel coding, so remember that the rate is a good indication of the performance of a system.\n",
    "\n",
    "The questions should be answered in the report. Please contact me when you think you have the answer so that I can follow your progress :)\n",
    "\n",
    "The goal is to finish section 1 and 2. Section 3 is optional. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull functions\n",
    "You do not have to use them all!\n",
    "\n",
    "- https://www.tensorflow.org/api_docs/python/tf/cast?hl=fr --> tensorflow operations can only be carried out on tensors that have the same type (int8, float32, complex64...)\n",
    "- https://www.tensorflow.org/api_docs/python/tf/shape\n",
    "- https://www.tensorflow.org/api_docs/python/tf/size?hl=fr\n",
    "- https://www.tensorflow.org/api_docs/python/tf/reshape\n",
    "- https://www.tensorflow.org/api_docs/python/tf/concat\n",
    "- https://www.tensorflow.org/api_docs/python/tf/one_hot\n",
    "- https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n",
    "- https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum\n",
    "- https://www.tensorflow.org/api_docs/python/tf/expand_dims\n",
    "- https://www.tensorflow.org/api_docs/python/tf/linalg/matmul\n",
    "- https://www.tensorflow.org/api_docs/python/tf/size?hl=fr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usefull jupyter notebook commands\n",
    "\n",
    "- `Ctrl + Enter` : Execute command\n",
    "- `Shift + Enter` : Execute command and go to next cell\n",
    "-  `dd` : Delete cell\n",
    "- 'b' : Create new cell below\n",
    "- 'a' : Create new cell above\n",
    "- The little arrow next to titles : group cells from the specific section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not intersting\n",
    "\n",
    "Do not look at this code, it is boring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize (model, SNRs_db):\n",
    "    K = model.K\n",
    "    msg = np.arange(2**K)\n",
    "    res = []\n",
    "    for i in range(2**K):\n",
    "        temp=msg[i]\n",
    "        res_ = []\n",
    "        for j in 2**np.linspace(K-1, 0, K):\n",
    "            if temp - j >= 0 :\n",
    "                res_.append(1)\n",
    "                temp -= j\n",
    "            else:\n",
    "                res_.append(0)\n",
    "        res.append(res_)\n",
    "    bits = np.array(res, dtype=np.float32) \n",
    "    \n",
    "    fig, axes = plt.subplots(1, len(SNRs_db), figsize=(len(SNRs_db)*7, 6))\n",
    "    if len(SNRs_db) == 1 :\n",
    "        axes = np.array([axes])\n",
    "    for i in range(len(SNRs_db)):\n",
    "                   \n",
    "        llrs = model(bits, tf.ones(shape=(tf.shape(bits)[0], 1))*SNRs_db[i])\n",
    "        x = model.x\n",
    "        if tf.size(tf.shape(x)) == 1 :\n",
    "            x = x[:, tf.newaxis]\n",
    "                   \n",
    "        axes[i].set_title('SNR_db = '+str(SNRs_db[i]))\n",
    "        for j in range(2**K):\n",
    "            axes[i].scatter(np.real(x[j, 0]), np.imag(x[j, 0]))\n",
    "            axes[i].text(np.real(x[j, 0])+0.02, np.imag(x[j, 0])+0.02, np.binary_repr(j, width=K), fontsize=8)\n",
    "            axes[i].set_xlim([-1.65, 1.65])\n",
    "            axes[i].set_ylim([-1.65, 1.65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_channel_uses (model, SNR_db):\n",
    "    K = model.K\n",
    "    msg = np.arange(2**K)\n",
    "    res = []\n",
    "    for i in range(2**K):\n",
    "        temp=msg[i]\n",
    "        res_ = []\n",
    "        for j in 2**np.linspace(K-1, 0, K):\n",
    "            if temp - j >= 0 :\n",
    "                res_.append(1)\n",
    "                temp -= j\n",
    "            else:\n",
    "                res_.append(0)\n",
    "        res.append(res_)\n",
    "    bits = np.array(res, dtype=np.float32) \n",
    "    llrs = model(bits, tf.ones(shape=(tf.shape(bits)[0], 1))*SNR_db)\n",
    "    \n",
    "    x = model.x\n",
    "    \n",
    "    if tf.size(tf.shape(x)) == 1 :\n",
    "        x = x[:, tf.newaxis]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, model.Nc, figsize=(model.Nc*7, 6))\n",
    "    if model.Nc == 1 :\n",
    "        axes = np.array([axes])\n",
    "    for i in range(model.Nc):\n",
    "        if model.Nc != 1 :\n",
    "            axes[i].set_title('Channel use n° '+str(i+1))\n",
    "        for j in range(2**K):\n",
    "            axes[i].scatter(np.real(x[j, i]), np.imag(x[j, i]))\n",
    "            axes[i].text(np.real(x[j, i])+.02, np.imag(x[j, i])+.02, np.binary_repr(j, width=K), fontsize=8)\n",
    "            axes[i].set_xlim([-1.65, 1.65])\n",
    "            axes[i].set_ylim([-1.65, 1.65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_gray(K):\n",
    "        def genCode(n):\n",
    "                if n == 0:\n",
    "                    return ['']\n",
    "\n",
    "                code1 = genCode(n-1)\n",
    "                code2 = []\n",
    "                for codeWord in code1:\n",
    "                    code2 = [codeWord] + code2\n",
    "\n",
    "                for i in range(len(code1)):\n",
    "                    code1[i] += '0'\n",
    "                for i in range(len(code2)):\n",
    "                    code2[i] += '1'\n",
    "                return code1 + code2  \n",
    "\n",
    "        k = K // 2 # bits per dimension\n",
    "        syms = 2**k\n",
    "        m = syms**2\n",
    "        # gray labeling\n",
    "        gray_labeling = []\n",
    "        for i in range(2**k):\n",
    "            gray_labeling.append(np.array([int(e) for e in  genCode(k)[i]]))\n",
    "        gray_labeling = np.array(gray_labeling)\n",
    "\n",
    "        encoding_table_256_qam = gray_labeling[:,::-1]\n",
    "\n",
    "        encoding_table_256_qam = np.hstack( [np.kron(encoding_table_256_qam,np.ones((syms,1))), np.tile(encoding_table_256_qam,(syms,1)) ])\n",
    "\n",
    "        natural_number = np.sum(2**np.arange(2*k)[::-1] * encoding_table_256_qam,axis=1)\n",
    "        table_gray = np.zeros((m,m))\n",
    "        for bvec in range(m):\n",
    "            table_gray[bvec,np.where(natural_number==bvec)[0]] =1\n",
    "\n",
    "        return table_gray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qam_const(K):\n",
    "    M = 2**K\n",
    "    size = int(np.sqrt(M))\n",
    "    a = np.arange(size)\n",
    "    b = a - np.mean(a)\n",
    "    C=[]\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            C.append(b[j]-1j*b[i])\n",
    "    C = C/np.sqrt(np.mean(np.square(np.abs(C))))\n",
    "    return C.astype(np.complex64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rate(model, SNRs_eval, DS_size=10000):\n",
    "    \n",
    "    print('K = '+str(model.K))\n",
    "    \n",
    "    DS_bits, DS_SNRs = gen_DS(DS_size, model.K, 1., 1.)\n",
    "    TF_DS_test = tf.data.Dataset.from_tensor_slices((DS_bits, DS_SNRs)).shuffle(DS_size).batch(1000)\n",
    "    \n",
    "    print(\"SNR (dB) = \", end='')\n",
    "    rates = []\n",
    "    for snr_i in SNRs_eval :\n",
    "\n",
    "        rates_temp = []\n",
    "        for step, (bits, SNRs) in enumerate(TF_DS_test):\n",
    "            pb_eq_1 = model(bits, SNRs*snr_i)\n",
    "            bits_pred = tf.sign(pb_eq_1-0.5)/2.+0.5\n",
    "            err_1 = tf.abs(bits - bits_pred)\n",
    "            err_2 = tf.abs(bits_pred - bits)\n",
    "            err_3 = tf.reduce_sum(tf.nn.relu(err_1 - err_2)) * tf.reduce_mean(tf.nn.tanh(err_2 - err_1))\n",
    "            err_4 =  tf.reduce_mean(tf.keras.losses.binary_crossentropy(bits, pb_eq_1)) * (tf.reduce_max(bits) + err_3)\n",
    "            rates_temp.append(model.K-err_4.numpy())\n",
    "        rates.append(np.mean(rates_temp))\n",
    "        print(str(snr_i)+', ', end='')\n",
    "    print('')\n",
    "    return rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could be interesting\n",
    "\n",
    "This could be interesting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_DS(DS_size, K, SNR_db_low, SNR_db_high):\n",
    "    \"\"\"Returns a dataset of dimension [epochs, Bs, K]\"\"\"\n",
    "        \n",
    "    DS_bits = tf.random.uniform(shape=[DS_size, K], minval=0, maxval=2, dtype=tf.int32)\n",
    "    DS_SNRs = tf.random.uniform(shape=[DS_size, 1], minval=SNR_db_low, maxval=SNR_db_high)\n",
    "    return tf.cast(DS_bits, tf.float32), DS_SNRs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not interesting\n",
    "\n",
    "Boring or \"complex\" stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mapper(Layer):\n",
    "    \"\"\"\n",
    "    Maps vector of K bits to a complex symbol\n",
    "    Inputs :\n",
    "    - K, the nber of bits/ch_uses\n",
    "    \"\"\"\n",
    "    def __init__(self, K, Nc, rayleigh):\n",
    "        super().__init__()     \n",
    "        \n",
    "        self.K = K\n",
    "        self.Nc = Nc\n",
    "        self.rayleigh = rayleigh\n",
    "        \n",
    "        if self.rayleigh == True :\n",
    "            self.Nc -= 1\n",
    "            if self.Nc < 1:\n",
    "                print('/!\\ with Rayleigh fading, Nc must be higher than one (Nc >= 2)')\n",
    "                raise Exception()\n",
    "        self.bin_conv = tf.convert_to_tensor(np.flip([np.power(2, i) for i in range(self.K)]), dtype=tf.float32)\n",
    "        \n",
    "    def call(self, bits, GM, QAM_const):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "        - bits = [Bs, K] \n",
    "        - GM, the Gray Mapping\n",
    "        - C = [M], the corresponding constallation\n",
    "        \n",
    "        Outputs:\n",
    "        - x = [Bs, Nc] \n",
    "        \"\"\"\n",
    "        self.bits = bits\n",
    "        self.bits_rshp = tf.reshape(bits, [-1, tf.shape(bits)[-1]])\n",
    "        \n",
    "        self.C_batch = tf.tile(tf.expand_dims(QAM_const, axis=0), [tf.shape(bits)[0], 1])\n",
    "        \n",
    "        self.s_dec = tf.cast(tf.reduce_sum(self.bits_rshp*self.bin_conv[tf.newaxis, :], axis=1), tf.int32) # [BS]\n",
    "        self.s_idx = tf.gather(GM, self.s_dec, axis=0)\n",
    "        self.s_idx = tf.argmax(self.s_idx, axis=1, output_type=tf.int32)\n",
    "        \n",
    "        self.s_oh = tf.one_hot(self.s_idx, depth=tf.pow(2, self.K))\n",
    "        self.C_exp = tf.reshape(tf.tile(self.C_batch[:, tf.newaxis, :], [1, 1, 1]), [-1, tf.pow(2, self.K)])[:, :, tf.newaxis]\n",
    "        \n",
    "        # Mapping symbols to complex points\n",
    "        self.x_rshp = tf.squeeze(tf.matmul(tf.cast(self.s_oh[:, tf.newaxis, :], tf.complex64), self.C_exp))\n",
    "        self.x = tf.tile(tf.reshape(self.x_rshp, [tf.shape(bits)[0], 1])[:, tf.newaxis, :], [1, self.Nc, 1])[:, :, 0]\n",
    "        \n",
    "        if self.rayleigh == True:\n",
    "            self.x = tf.concat([self.x, tf.ones([tf.shape(self.x)[0], 1], dtype=tf.complex64)], axis=1)\n",
    "            \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel(Layer):\n",
    "\n",
    "    def __init__(self, Nc=1, rayleigh=False, **kwargs):\n",
    "        super(Channel, self).__init__(**kwargs)\n",
    "        \n",
    "        self.Nc = Nc\n",
    "        self.rayleigh = rayleigh\n",
    "        \n",
    "    def call(self, x, SNR_db):\n",
    "        \n",
    "        if tf.reduce_mean(tf.square(tf.abs(x)))>2.5:\n",
    "            tf.print('Watch out! You mean power should be one, but is actually ' + str(tf.reduce_mean(tf.square(tf.abs(x))).numpy()))\n",
    "            raise Exception ('Stop due to wrong average symbol power')\n",
    "        \n",
    "        dims_1 = False\n",
    "        if tf.size(tf.shape(x)) == 1 :\n",
    "            x = x[tf.newaxis]\n",
    "            dims_1 = True\n",
    "            \n",
    "        if self.rayleigh == False:\n",
    "            noise_std = tf.sqrt( 1 / tf.pow(10., SNR_db/10.0))\n",
    "            noise_r = tf.random.normal(shape = [tf.shape(x)[0], self.Nc], stddev = 1) * noise_std/tf.sqrt(2.)\n",
    "            noise_i = tf.random.normal(shape = [tf.shape(x)[0], self.Nc], stddev = 1) * noise_std/tf.sqrt(2.)\n",
    "            self.noise_cplx = tf.complex(noise_r, noise_i)\n",
    "        \n",
    "            self.y = x + self.noise_cplx\n",
    "        \n",
    "        else:\n",
    "            rayleigh_real = tf.random.normal(shape = [tf.shape(x)[0]], stddev = 1./tf.sqrt(2.))\n",
    "            rayleigh_imag = tf.random.normal(shape = [tf.shape(x)[0]], stddev = 1./tf.sqrt(2.))\n",
    "            self.h = tf.tile(tf.complex(rayleigh_real, rayleigh_imag)[:, tf.newaxis], [1, self.Nc])\n",
    "\n",
    "            noise_std = tf.sqrt( 1 / tf.pow(10., SNR_db/10.0))\n",
    "            noise_r = tf.random.normal(shape = [tf.shape(x)[0], self.Nc], stddev = 1) * noise_std/tf.sqrt(2.)\n",
    "            noise_i = tf.random.normal(shape = [tf.shape(x)[0], self.Nc], stddev = 1) * noise_std/tf.sqrt(2.)\n",
    "            self.noise_cplx = tf.complex(noise_r, noise_i)\n",
    "\n",
    "            self.y = self.h*x + self.noise_cplx\n",
    "        \n",
    "        if dims_1 == True :\n",
    "            self.y = self.y[:, 0]\n",
    "            \n",
    "        return self.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Demapper(Layer):\n",
    "\n",
    "    def __init__(self, K, GM, rayleigh, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.K = K\n",
    "        self.const_size = 2**K\n",
    "        self.rayleigh = rayleigh\n",
    "        \n",
    "        self.bits2symb = np.argmax(GM, axis=1)\n",
    "        bit_list = np.arange(self.const_size)\n",
    "        bit_list = [[int(j) for j in list(np.binary_repr(i, width=self.K))] for i in bit_list]\n",
    "        self.bits_nat = np.array(bit_list)\n",
    "\n",
    "        self.ind_all = np.arange(self.const_size, dtype=np.int64)\n",
    "\n",
    "        self.B = []\n",
    "        for i in range(self.K):\n",
    "            self.B.append([None, None])\n",
    "            # 0\n",
    "            a = np.where(self.bits_nat[:,i] == 0)[0]\n",
    "            self.B[i][0] = np.take(self.bits2symb, a)\n",
    "            # 1\n",
    "            a = np.where(self.bits_nat[:,i] == 1)[0]\n",
    "            self.B[i][1] = np.take(self.bits2symb, a)\n",
    "    \n",
    "    \n",
    "    def awgn_cde(self, snr_db, y, s, C):\n",
    "        mu = tf.gather(C, s, axis=1)\n",
    "        snr = tf.pow(tf.cast(10.0, tf.float32), snr_db/10.0)\n",
    "        v = 1./(snr)\n",
    "        z = -tf.reduce_sum(tf.square(y - mu), axis=2)/v\n",
    "        \n",
    "        return z\n",
    "    \n",
    "    def call(self, y_cplx, SNR_db, QAM_const):\n",
    "        \"\"\"\n",
    "        AWGN demapper for SISO links\n",
    "        \n",
    "        Inputs : y_cplx = [Bs, Nc], snr_db = [1], C = [M]\n",
    "        Outputs : LLRs = [Bs, K]\n",
    "        \"\"\"\n",
    "        if self.rayleigh==True:\n",
    "            y_cplx_eq = y_cplx[:, :-1] / y_cplx[:, -1:]\n",
    "            y_cplx_mean = tf.reduce_mean(y_cplx_eq, axis=1, keepdims=True)\n",
    "        else :\n",
    "            y_cplx_mean = tf.reduce_mean(y_cplx, axis=1, keepdims=True)\n",
    "            \n",
    "        SNR_db_batch = SNR_db #tf.ones([tf.shape(y_cplx_mean)[0], 1]) * SNR_db\n",
    "        C_batch = tf.tile(tf.expand_dims(QAM_const, axis=0), [tf.shape(y_cplx_mean)[0], 1])\n",
    "        \n",
    "        C_r_i = tf.stack([tf.math.real(C_batch), tf.math.imag(C_batch)], axis=2)\n",
    "        y = tf.stack([tf.math.real(y_cplx_mean), tf.math.imag(y_cplx_mean)], axis=2) \n",
    "        \n",
    "        pyb_0 = []\n",
    "        pyb_1 = []\n",
    "        y = tf.tile(y, [1, self.const_size//2, 1])\n",
    "        y_2 = tf.tile(y, [1, 2, 1])\n",
    "        p = tf.constant(self.ind_all, tf.int32)\n",
    "        z_all = self.awgn_cde(SNR_db_batch, y_2, p, C_r_i)\n",
    "        for i in range(self.K):\n",
    "            # 0\n",
    "            p = tf.constant(self.B[i][0], tf.int32)\n",
    "            z = tf.gather(z_all, p, axis=1)\n",
    "            z = tf.math.reduce_logsumexp(z, axis=1)\n",
    "            pyb_0.append(z)\n",
    "            # 1\n",
    "            p = tf.constant(self.B[i][1], tf.int32)\n",
    "            z = tf.gather(z_all, p, axis=1)\n",
    "            z = tf.math.reduce_logsumexp(z, axis=1)\n",
    "            pyb_1.append(z)\n",
    "        pyb_0 = tf.stack(pyb_0, axis=1)\n",
    "        pyb_1 = tf.stack(pyb_1, axis=1)\n",
    "        llr = pyb_1 - pyb_0\n",
    "        \n",
    "        return llr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Could be interesting\n",
    "\n",
    "You might want to look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_QAM(Model) :\n",
    "    def __init__(self, K, Nc=1, rayleigh=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.K = K\n",
    "        self.Nc = Nc \n",
    "        self.QAM_const = tf.constant(qam_const(K))\n",
    "        self.GM = tf.constant(table_gray(K), tf.float32)\n",
    "\n",
    "        self.mapper = Mapper(self.K, self.Nc, rayleigh)\n",
    "        self.channel = Channel(self.Nc, rayleigh)\n",
    "        self.demapper = Demapper(self.K, self.GM, rayleigh)\n",
    "\n",
    "    #@tf.function\n",
    "    def call(self, bits, SNR_db):\n",
    "        \n",
    "        self.bits = tf.cast(bits, tf.float32)\n",
    "        \n",
    "        self.x = self.mapper(bits, self.GM, self.QAM_const)\n",
    "            \n",
    "        self.y = self.channel(self.x, SNR_db)\n",
    "\n",
    "        self.llrs = self.demapper(self.y, SNR_db, self.QAM_const)\n",
    "        \n",
    "        self.pb_eq_1 = tf.nn.sigmoid(self.llrs)\n",
    "        \n",
    "        return self.pb_eq_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline (QAM) system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/mgoutay/ml_course/blob/master/Images/standard_system.png?raw=true\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QAM transmission\n",
    "\n",
    "Let's start by defining the baseline that we want to beat : a 64-QAM transmitter/receiver\n",
    "\n",
    "The number of bits represented by one complex symbol is denoted $K$\n",
    "\n",
    "Using a 64-QAM transmitter means that we use one complex symbol to transmit 6 bits ($K=6$)\n",
    "\n",
    "- choosing $K=2$ means that QPSK (= 4-QAM) is used\n",
    "- choosing $K=4$ means that 16-QAM is used\n",
    "- etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notations\n",
    "\n",
    "(scalars are denoted by lower-case letters, vectors by bold lower-case letters)\n",
    "\n",
    "The bits that we want to send over the channel are denoted $\\mathbf{b}\\in\\{0, 1\\}^{K}$. It is a vector of bits (0, 1) of size $K$.\n",
    "\n",
    "The corresponding complex symbol sent by the transmitter is denoted $x\\in\\mathbb{C}$, and the symbol received by the receiver is denoted $y\\in\\mathbb{C}$\n",
    "\n",
    "Finally, the receiver outputs probabilities $\\mathbf{p}\\in\\mathbb{R}^K$. Each entry $p_i$ of $\\mathbf{p}$ correspond to the probability that the $i^{\\text{th}}$ bit equals one, so we have $0 \\leq p_i \\leq 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Channel model\n",
    "\n",
    "We consider a Additive White Gaussian Noise (AWGN) channel, where we have: \n",
    "$$y = x + n$$\n",
    "$n$ being complex gaussian noise with variance $\\sigma^2$ : $n \\sim \\mathcal{C}\\mathcal{N}(0, \\sigma^2) $, meaning that the variance of both the real and imaginary part is $\\frac{\\sigma^2}{2}$.\n",
    "\n",
    "It is assumed that $\\mathbb{E}[|x|^2] =1$, i.e., the average power per symbol is one.\n",
    "\n",
    "The Signal to Noise Ratio (SNR) of the transmission is defined as $\\text{SNR} = \\frac{\\mathbb{E}[|x|^2]}{\\sigma^2} = \\frac{1}{\\sigma^2} $. It is common practice to represent the SNR in decibel : $\\text{SNR}_{\\text{dB}} = 10 \\text{log}_{10} (\\text{SNR})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=6\n",
    "DS_size=10000 #Size of the Dataset to generate\n",
    "\n",
    "DS_bits, DS_SNRs_db = gen_DS(DS_size, K, SNR_db_low=-5, SNR_db_high=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "The datasets are generated by the `gen_DS` function, which returns a dataset composed of random bits of dimension [DS_size, K] and random SNRs of dimension [DS_size, 1] in dB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS_bits.shape)\n",
    "print(DS_SNRs_db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(DS_bits[:2]) #Display only the first two entries in the dataset\n",
    "print(DS_SNRs_db[:2]) #Display only the first two entries in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now initialize our baseline model, and make a simple forward pass through it. The output of the model is the estimated probabilities that the received bits are ones. Let's try with the first two sample of our dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the baseline\n",
    "baseline = Model_QAM(K)\n",
    "\n",
    "# Define a very small batch just just to visualize the output\n",
    "batch_bits = DS_bits[:2]\n",
    "batch_SNRs = DS_SNRs_db[:2]\n",
    "\n",
    "# Print the sent bits\n",
    "print(batch_bits)\n",
    "\n",
    "# Compute probabilities that the bits equals to one\n",
    "prob_eq_1 = baseline(batch_bits, batch_SNRs)\n",
    "\n",
    "# Print the symbols that were sent\n",
    "print(baseline.x)\n",
    "\n",
    "# Print the predicted probabilities\n",
    "print(prob_eq_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vizualize the symbol sent at different SNRs (and the associated bits) using the `visualize()` function : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(baseline, SNRs_db=[0, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see something special about the way the symbols are attanged?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the rate of a transmission is the maximum number of bits that could be sent through the transmitter-receiver if perfect channel coding was used (the bits are coded/decoded before/after the transmissions). In this project, we won't use channel coding, so remember that the rate is a good indication of the performance of a system. The computation of the rate is a bit complex so do not try to undersand it ;)\n",
    "\n",
    "We can evaluate the rate of the transmission at different SNRs using the `evaluate_rate()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRs_eval = np.linspace(0, 25, 6)\n",
    "rates_baseline = evaluate_rate(baseline, SNRs_eval)\n",
    "\n",
    "plt.plot(SNRs_eval, rates_baseline)\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Rate')\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the rates we want to beat! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Get your hands dirty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/mgoutay/ml_course/blob/master/Images/NN-based_system.png?raw=true\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section, the goal is to create your first autoencoder. To make trainings faster, we set $K = 4$ here. Do not try to beat the baseline, as it might be hard for $K<6$\n",
    "\n",
    "The main objectives are to :\n",
    "- Define a loss function\n",
    "- Create the NN-based transmitter and receiver \n",
    "- To normalize the output so that the average power per symbol is one\n",
    "- Create a training loop\n",
    "- Train, evaluate, and compare your system to the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to make sure that your average power per symbol is one !\n",
    "You can compute your average energy by computing `tf.reduce_mean(tf.square(tf.abs(x)))` once you have performed at least one forward path (so that self.x  contains some values)\n",
    "\n",
    "$$\\mathbb{E}[|x|^2] \\approx \\frac{1}{B_s} \\sum_{i=1}^{B_s} |x|^2 $$\n",
    "\n",
    "How can you make sure that $\\frac{1}{B_s} \\sum_{i=1}^{B_s} |x|^2 = 1$ ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 - Import the layer(s) you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import the layer(s) you need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 -  Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is your autoencoder model, comprising the transmitter, channel, and receiver\n",
    "class Autoencoder(Model):\n",
    "\n",
    "    def __init__(self, K, **kwargs):\n",
    "        super(Autoencoder, self).__init__(**kwargs)\n",
    "        \n",
    "        self.K = K\n",
    "        #You can set other attributes\n",
    "        \n",
    "        self.tx_layer_1 = ### Define transmitter layers\n",
    "        self.tx_layer_2 = ### Define transmitter layers\n",
    "        self.tx_layer_3 = ### Define transmitter layers\n",
    "        ### ...\n",
    "        \n",
    "        self.channel = Channel() \n",
    "        \n",
    "        self.rx_layer_1 = ### Define receiver layers\n",
    "        self.rx_layer_2 = ### Define receiver layers\n",
    "        self.rx_layer_3 = ### Define receiver layers\n",
    "        ### ...\n",
    "    \n",
    "    def call(self, bits, SNR_db):\n",
    "        \n",
    "        ### Use layers and/or other Tensorflow functions\n",
    "        self.x = # This must be the symbols that are sent\n",
    "            \n",
    "        self.y = self.channel(self.x, SNR_db)\n",
    "        \n",
    "        ### Use layers and/or other Tensorflow functions\n",
    "        self.pb = # This must be the probabilities that the sent bits are ones\n",
    "\n",
    "        return self.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 -  Instantiate the model, the loss, and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 4\n",
    "\n",
    "\n",
    "autoencoder = Autoencoder(K) # Instantiate an autoencoder\n",
    "baseline = Model_QAM(K) # Instantiate a baseline with the same K for comparison\n",
    "\n",
    "#Initialize optimizer and loss function\n",
    "optimizer = # Instantiate an optimizer\n",
    "loss_func = # Pick a loss function that makes sense for this problem, do not try to understand how the rate is computed! If you choose a good loss funciton, the rate of the transmission will be maximized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What loss function did you use and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - generate and create optimized Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_size = 100000\n",
    "\n",
    "batch_size = 1024\n",
    "DS_bits, DS_SNRs_db = gen_DS(DS_size, K, SNR_db_low=-5, SNR_db_high=30)\n",
    "\n",
    "TF_DS_train = # Create optimized TF dataset\n",
    "TF_DS_test = # Create optimized TF dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 - Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training loop "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Section 1, the goal is to have a model that works. You can evaluate the model with the `evaluate_rate()` function, but do not try to beat the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SNRs (dB) at which we want to evaluate the models\n",
    "SNRs_eval = np.linspace(0, 20, 5)\n",
    "\n",
    "# Evaluate the models\n",
    "rate_autoencoder = evaluate_rate(autoencoder, SNRs_eval)\n",
    "rate_baseline = evaluate_rate(baseline, SNRs_eval) # For comparison\n",
    "\n",
    "#plot the result\n",
    "plt.plot(SNRs_eval, rate_baseline, label='Baseline')\n",
    "plt.plot(SNRs_eval, rate_autoencoder, label='Autoencoder')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can vizualize the learned constellations at different SNRs:\n",
    "\n",
    "What do you see (two things should be noticed)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(baseline, SNRs_db=[0, 5, 10, 20]) #Visualize the symbols generated by the conventional QAM system\n",
    "visualize(autoencoder, SNRs_db=[0, 5, 10, 20]) #Visualize the symbols generated by the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Send three batches of random bits and plot the sent symbols for a fixed SNR of 10dB:\n",
    "\n",
    "What do you see ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRs =  tf.ones((100, 1))*10.\n",
    "\n",
    "rdm_bits = tf.cast(tf.random.uniform([100, K], 0, 2, dtype=tf.int32), tf.float32)\n",
    "pb = autoencoder(rdm_bits, SNRs)\n",
    "plt.scatter(np.real(autoencoder.x), np.imag(autoencoder.x), marker='+', label='1st transmission')\n",
    "\n",
    "rdm_bits = tf.cast(tf.random.uniform([100, K], 0, 2, dtype=tf.int32), tf.float32)\n",
    "pb = autoencoder(rdm_bits, SNRs)\n",
    "plt.scatter(np.real(autoencoder.x), np.imag(autoencoder.x), marker='+', label='2nd transmission')\n",
    "\n",
    "rdm_bits = tf.cast(tf.random.uniform([100, K], 0, 2, dtype=tf.int32), tf.float32)\n",
    "pb = autoencoder(rdm_bits, SNRs)\n",
    "plt.scatter(np.real(autoencoder.x), np.imag(autoencoder.x), marker='+', label='3rd transmission')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate a proper constellation\n",
    "\n",
    "Here we want to solve the problem that we have spotted just above. Please call me before starting Section 2 so that I can make sure that you understand the problem.\n",
    "\n",
    "The idea here is to use the transmitter to generate constellation vectors that depends on the SNR, instead of directly generating the symbols to send. For one entry in the batch (one SNR), this constellation vector contains all $2^K$ symbols that can be sent, so it is a complex vector of size $2^K$.\n",
    "\n",
    "To generate the sent symbols, one can then:\n",
    "- For each entries in the batch, convert the vector of bits to be send to their one-hot representation. For example, with $K=2$ : [00] = [1000], [10] = [0100], [01] = [0010], [11] = [0001] or something similar.  You might want to use `tf.pow()` and `tf.one_hot()`\n",
    "- Multiply (element-wise) the one-hot representation of the bits to the corresponding constellation vector generated by the transmitter\n",
    "- The resulting vector is of size $2^K$ but only one entry is non-zero. This can be simplified to a single complex number, for example using `tf.reduce_sum()` on the good axis\n",
    "\n",
    "What is the benefit of using this technique?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder2(Model):\n",
    "\n",
    "    def __init__(self, K, **kwargs):\n",
    "        super(Autoencoder2, self).__init__(**kwargs)\n",
    "        \n",
    "        self.K = K\n",
    "        #You can set other attributes\n",
    "        \n",
    "        self.tx_layer_1 = ### Define transmitter layers\n",
    "        self.tx_layer_2 = ### Define transmitter layers\n",
    "        self.tx_layer_3 = ### Define transmitter layers\n",
    "\n",
    "        self.channel = Channel()\n",
    "        \n",
    "        self.rx_layer_1 = ### Define receiver layers\n",
    "        self.rx_layer_2 = ### Define receiver layers\n",
    "        self.rx_layer_3 = ### Define receiver layers\n",
    "    \n",
    "    def call(self, bits, SNR_db):\n",
    "        \n",
    "        ### Use layers and/or other Tensorflow functions\n",
    "        self.x = # This must be the symbols that are sent\n",
    "            \n",
    "        self.y = self.channel(self.x, SNR_db)\n",
    "        \n",
    "        ### Use layers and/or other Tensorflow functions\n",
    "        self.pb = # This must be the probabilities that the sent bits are ones\n",
    "\n",
    "        return self.pb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can start by setting $K=4$, but remember to use $K=6$ when you try to beat the baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 -  Instantiate the model, the loss, and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "\n",
    "# Instantiate an autoencoder\n",
    "autoencoder2 = Autoencoder2(K)\n",
    "baseline2 = Model_QAM(K)\n",
    "\n",
    "#Initialize optimizer and loss function\n",
    "optimizer = # Optimizer\n",
    "loss_func = # Loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - generate and create optimized Tensorflow datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_size = 100000\n",
    "batch_size = 1024\n",
    "\n",
    "DS_bits, DS_SNRs_db = gen_DS(DS_size, K, SNR_db_low=-5, SNR_db_high=25)\n",
    "print('Dataset generated with K =', K)\n",
    "\n",
    "TF_DS_train = # Create optimized TF dataset\n",
    "TF_DS_test = # Create optimized TF dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you manage to beat the baseline with $K=6$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRs_eval = np.linspace(0, 20, 5)\n",
    "rate_autoencoder2 = evaluate_rate(autoencoder2, SNRs_eval)\n",
    "rate_baseline2 = evaluate_rate(baseline2, SNRs_eval)\n",
    "\n",
    "plt.plot(SNRs_eval, rate_baseline2, label='Baseline2')\n",
    "plt.plot(SNRs_eval, rate_autoencoder2, label='Autoencoder2')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the sent constellation at different SNRs : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize(autoencoder2, SNRs_db=[0, 5, 10, 20])\n",
    "visualize(baseline2, SNRs_db=[0, 5, 10, 20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Transmit on multiple channel uses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vector of $K$ bits can also be transmitted on multiple channel uses, meaning that for each vector of $K$ bits in the batch we send $N_c$ complex symbols instead of $1$. $N_c$ denotes the number of channel uses, and the symbols sent that correspond to one vector of bits are denoted $\\mathbf{x}\\in \\mathbb{C}^{N_c}$\n",
    "\n",
    "The goal here is to transmit $N_c$ symbols for each entry in the batch, making sure that the average power per symbol remains one : $\\mathbb{E}[[x|] =1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 - Example with the baseline, with $N_c=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K=6\n",
    "Nc = 2\n",
    "\n",
    "DS_size=100 \n",
    "DS_bits, DS_SNRs_db = gen_DS(DS_size, K, SNR_db_low=-5, SNR_db_high=30) #Generate a dataset\n",
    "\n",
    "# Initialize the baseline\n",
    "baseline3 = Model_QAM(K, Nc)\n",
    "\n",
    "# Define a very small batch containing just one sample to visualize the output\n",
    "batch_bits = DS_bits[:1]\n",
    "batch_SNRs = DS_SNRs_db[:1]\n",
    "\n",
    "# Print the sent bits\n",
    "print('batch_bits : ', batch_bits)\n",
    "print('batch_SNRs : ', batch_SNRs)\n",
    "\n",
    "# Compute probabilities that the bits equals to one\n",
    "prob_eq_1 = baseline3(batch_bits, batch_SNRs)\n",
    "\n",
    "# Print the symbols that were sent\n",
    "print('baseline.x : ', baseline3.x) # x is now a vector of size Nc !\n",
    "\n",
    "# Print the predicted probabilities\n",
    "print('pb predicted by the baseline : ', prob_eq_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can visualize the $N_c$ channel uses with the `visualize_channel_uses()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_channel_uses(baseline3, SNR_db = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Build the autoencoder, train, and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder3(Model):\n",
    "\n",
    "    def __init__(self, K, Nc, **kwargs):\n",
    "        super(Autoencoder3, self).__init__(**kwargs)\n",
    "        \n",
    "        self.K = K\n",
    "        self.Nc = Nc\n",
    "        #You can set other attributes\n",
    "        \n",
    "        self.tx_layer_1 = ### Define transmitter layers\n",
    "        self.tx_layer_2 = ### Define transmitter layers\n",
    "        self.tx_layer_3 = ### Define transmitter layers\n",
    "\n",
    "        self.channel = Channel(self.Nc)\n",
    "        \n",
    "        self.rx_layer_1 = ### Define receiver layers\n",
    "        self.rx_layer_2 = ### Define receiver layers\n",
    "        self.rx_layer_3 = ### Define receiver layers\n",
    "    \n",
    "    def call(self, bits, SNR_db):\n",
    "        \n",
    "        ### Use layers and/or other Tensorflow functions\n",
    "        self.x = # This must be the symbols that are sent\n",
    "            \n",
    "        self.y = self.channel(self.x, SNR_db)\n",
    "        \n",
    "        ### Use layers and/or other Tensorflow functions\n",
    "        self.pb = # This must be the probabilities that the sent bits are ones\n",
    "\n",
    "        return self.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 6\n",
    "Nc = 2\n",
    "\n",
    "# Instantiate an autoencoder\n",
    "autoencoder3 = Autoencoder3(K, Nc)\n",
    "baseline3 = Model_QAM(K, Nc)\n",
    "\n",
    "#Initialize optimizer and loss function\n",
    "optimizer =  ### Optimizer\n",
    "loss_func = ### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_size = 100000\n",
    "batch_size = 1024\n",
    "DS_bits, DS_SNRs_db = gen_DS(DS_size, K, SNR_db_low=-5, SNR_db_high=25)\n",
    "print('Dataset generated with K =', K)\n",
    "\n",
    "TF_DS_train = # Create optimized TF dataset\n",
    "TF_DS_test = # Create optimized TF dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNRs_eval = np.linspace(0, 20, 5)\n",
    "rate_autoencoder3 = evaluate_rate(autoencoder3, SNRs_eval)\n",
    "rate_baseline3 = evaluate_rate(baseline3, SNRs_eval)\n",
    "\n",
    "plt.plot(SNRs_eval, rate_baseline3, label='Baseline3')\n",
    "plt.plot(SNRs_eval, rate_autoencoder3, label='Autoencoder3')\n",
    "plt.xlabel('SNR')\n",
    "plt.ylabel('Rate')\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_channel_uses(autoencoder3, SNR_db = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you see something interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
